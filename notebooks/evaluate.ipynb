{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-26T23:13:45.189865Z",
     "start_time": "2025-11-26T23:13:38.099693Z"
    }
   },
   "source": [
    "from tools.checkpoint import model_from_checkpoint\n",
    "from training.eval import Evaluator\n",
    "from data.data_gen_stream import DistributedDataGenerator\n",
    "\n",
    "ddg = DistributedDataGenerator(\n",
    "    filename_pattern=\"/Users/jonathanmiddleton/projects/daisy/data/dclm_baseline/dclm_baseline_val_000000.bin\",\n",
    "    batch_size=8192,\n",
    "    rank=0,\n",
    "    world_size=1,\n",
    "    start_shard=1,\n",
    "    device='mps',\n",
    ")\n",
    "evaluator = Evaluator(\n",
    "            data_generator=ddg,\n",
    "            distributed_enabled=False,\n",
    "            rank=0,\n",
    "            val_type='pretraining'\n",
    ")\n",
    "\n",
    "model, _ = model_from_checkpoint(\n",
    "    '/Users/jonathanmiddleton/models/checkpoints/650m-base/20251126T2111-val3.410-step016640-tokens6815744000-run1-best.pt',\n",
    "    device='mps')\n",
    "model.eval()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-26 18:13:40,334 WARNING tools.master_logger: [MasterLogger] Falling back to console logging; failed to apply config/logging.yml: [Errno 2] No such file or directory: 'config/logging.yml'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DaisyCore(\n",
       "  (embed): Embedding(50257, 1024)\n",
       "  (zero_embedding): ZeroEmbedding()\n",
       "  (value_embeds): ModuleList(\n",
       "    (0): Embedding(50257, 1024)\n",
       "    (1): ZeroEmbedding()\n",
       "    (2): Embedding(50257, 1024)\n",
       "    (3-4): 2 x ZeroEmbedding()\n",
       "    (5): Embedding(50257, 1024)\n",
       "    (6-7): 2 x ZeroEmbedding()\n",
       "    (8): Embedding(50257, 1024)\n",
       "    (9): ZeroEmbedding()\n",
       "    (10): Embedding(50257, 1024)\n",
       "    (11-12): 2 x ZeroEmbedding()\n",
       "    (13): Embedding(50257, 1024)\n",
       "    (14): ZeroEmbedding()\n",
       "    (15): Embedding(50257, 1024)\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-2): 3 x Block(\n",
       "      (attn): CausalSelfAttention(\n",
       "        (rotary): Rotary()\n",
       "      )\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (4-5): 2 x Block(\n",
       "      (attn): CausalSelfAttention(\n",
       "        (rotary): Rotary()\n",
       "      )\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (7-8): 2 x Block(\n",
       "      (attn): CausalSelfAttention(\n",
       "        (rotary): Rotary()\n",
       "      )\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (10-11): 2 x Block(\n",
       "      (attn): CausalSelfAttention(\n",
       "        (rotary): Rotary()\n",
       "      )\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (12): Block(\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "    (13-15): 3 x Block(\n",
       "      (attn): CausalSelfAttention(\n",
       "        (rotary): Rotary()\n",
       "      )\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T23:22:54.230528Z",
     "start_time": "2025-11-26T23:13:45.195694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_tokens = 40_000_000\n",
    "val_loss = evaluator.eval(model=model, total_tokens=total_tokens)\n",
    "val_loss"
   ],
   "id": "d3ecdb23cfbf027d",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m total_tokens = \u001B[32m40_000_000\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m val_loss = \u001B[43mevaluator\u001B[49m\u001B[43m.\u001B[49m\u001B[43meval\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtotal_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtotal_tokens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      3\u001B[39m val_loss\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/daisy/training/eval.py:224\u001B[39m, in \u001B[36mEvaluator.eval\u001B[39m\u001B[34m(self, model, total_tokens)\u001B[39m\n\u001B[32m    222\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(steps - \u001B[32m1\u001B[39m):\n\u001B[32m    223\u001B[39m     inputs, targets = \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m._ddg)\n\u001B[32m--> \u001B[39m\u001B[32m224\u001B[39m     \u001B[43mrun_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    226\u001B[39m \u001B[38;5;66;03m# Average across ranks\u001B[39;00m\n\u001B[32m    227\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._distributed_enabled:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/daisy/training/eval.py:210\u001B[39m, in \u001B[36mEvaluator.eval.<locals>.run_step\u001B[39m\u001B[34m(x, y)\u001B[39m\n\u001B[32m    208\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n\u001B[32m    209\u001B[39m         n_blocks = torch.tensor(\u001B[32m1\u001B[39m, dtype=torch.int32, device=device)\n\u001B[32m--> \u001B[39m\u001B[32m210\u001B[39m         loss = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_blocks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._val_type == \u001B[33m'\u001B[39m\u001B[33mpretraining\u001B[39m\u001B[33m'\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m model(x, n_blocks, y, loss_chunks=\u001B[32m1\u001B[39m)\n\u001B[32m    212\u001B[39m \u001B[38;5;66;03m# Optional per-sample debug logging\u001B[39;00m\n\u001B[32m    213\u001B[39m \u001B[38;5;28mself\u001B[39m._log_sample(step_idx, loss, x, y)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/daisy-wee/lib/python3.12/site-packages/torch/nn/modules/module.py:1783\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1783\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/daisy-wee/lib/python3.12/site-packages/torch/nn/modules/module.py:1794\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1789\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1790\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1791\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1792\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1793\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1794\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1796\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1797\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/daisy/models/daisy/daisy_core.py:260\u001B[39m, in \u001B[36mDaisyCore.forward\u001B[39m\u001B[34m(self, input_seq, sliding_window_num_blocks, target_seq, loss_chunks, output_logits)\u001B[39m\n\u001B[32m    258\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m skip_map:\n\u001B[32m    259\u001B[39m         x = x + skip_weights[skip_map[i]] * skip_connections[skip_map[i]]\n\u001B[32m--> \u001B[39m\u001B[32m260\u001B[39m     x = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mblocks\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mve\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlambdas\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msa_lambdas\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mblock_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mblock_masks\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    261\u001B[39m     skip_connections.append(x)\n\u001B[32m    263\u001B[39m x = norm(x)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/daisy-wee/lib/python3.12/site-packages/torch/nn/modules/module.py:1783\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1783\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/daisy-wee/lib/python3.12/site-packages/torch/nn/modules/module.py:1794\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1789\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1790\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1791\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1792\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1793\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1794\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1796\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1797\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/daisy/models/daisy/block.py:56\u001B[39m, in \u001B[36mBlock.forward\u001B[39m\u001B[34m(self, x, ve, x0, lambdas, sa_lambdas, block_mask, attn_mask)\u001B[39m\n\u001B[32m     54\u001B[39m x = lambdas[\u001B[32m0\u001B[39m] * x + lambdas[\u001B[32m1\u001B[39m] * x0\n\u001B[32m     55\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.attn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m56\u001B[39m     x = x + \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mattn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mve\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msa_lambdas\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mblock_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mblock_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     57\u001B[39m x = x + \u001B[38;5;28mself\u001B[39m.mlp(norm(x))\n\u001B[32m     58\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/daisy-wee/lib/python3.12/site-packages/torch/nn/modules/module.py:1783\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1783\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/Caskroom/miniconda/base/envs/daisy-wee/lib/python3.12/site-packages/torch/nn/modules/module.py:1794\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1789\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1790\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1791\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1792\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1793\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1794\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1796\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1797\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/daisy/models/daisy/attention.py:146\u001B[39m, in \u001B[36mCausalSelfAttention.forward_sdpa\u001B[39m\u001B[34m(self, x, ve, sa_lambdas, attn_mask, block_mask)\u001B[39m\n\u001B[32m    145\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward_sdpa\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: torch.Tensor, ve: torch.Tensor, sa_lambdas: torch.Tensor, attn_mask: Tensor, block_mask: Optional[Tensor] = \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m--> \u001B[39m\u001B[32m146\u001B[39m     y, _, _, _ = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sdpa_common\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mve\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msa_lambdas\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    147\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m y\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/daisy/models/daisy/attention.py:138\u001B[39m, in \u001B[36mCausalSelfAttention._sdpa_common\u001B[39m\u001B[34m(self, x, ve, sa_lambdas, attn_mask)\u001B[39m\n\u001B[32m    136\u001B[39m q_, k_, v_ = \u001B[38;5;28mself\u001B[39m._qkv_common(x, ve, sa_lambdas)\n\u001B[32m    137\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m attn_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m138\u001B[39m     y = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mnn\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfunctional\u001B[49m\u001B[43m.\u001B[49m\u001B[43mscaled_dot_product_attention\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mattn_scale\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    139\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    140\u001B[39m     y = torch.nn.functional.scaled_dot_product_attention(q_, k_, v_, is_causal=\u001B[38;5;28;01mTrue\u001B[39;00m, scale=\u001B[38;5;28mself\u001B[39m.attn_scale)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
